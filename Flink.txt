[TOC]
#Flink
##1.Introduction
    流处理和批处理通常被认为是两种类型非常不同的处理系统；
    “Lambda architecture”：“整合”批处理和流处理；
    Flink用数据流处理“统一”了实时分析、流处理、批处理的编程模型和执行引擎；
        批处理是流处理的特例（有限流）
        持久的消息队列、数据流重放
        高度灵活的窗口机制


##2.系统架构
    streams连接起来的有状态的operators的DAG。
    DataSet和DataStream API都运行在流数据流引擎上；
    
    Client：
        转换程序代码为数据流图；
        对DataSet程序做基于代价的查询优化；
    JobManager：
        跟踪每个operator和stream的状态和进度；
        调度新的operator；
        协调检查点和容错恢复；
    TaskManager：
        执行operators；
        报告状态给JobManager；
        心跳；
        缓冲或物化streams的缓冲池；
        operators之间交换data streams的网络连接；
    
    架构和 storm一样没有资源隔离。
    
    Flink4个执行图（StreamGraph,JobGraph(整合StreamGraph和OptimizedGraph),ExecutionGraph,物理执行图）和spark（数据依赖图,物理执行DAG）一样解耦合。



##3.Streaming Dataflows
###3.1 Dataflow图
    Dataflow Graph：a DAG consists of stateful operators and data streams;
        operators并行成一个或多个并行实例“subtask”；
        streams被切分成一个或多个stream分区（一个subtask对应一个stream分区）；

###3.2 中间Data Stream的数据交换
    1.Pipelined and Blocking Data Exchange
        pipelined intermediate streams提供从消费者到生产者的后压（back pressure）+ 减少物化；
        Blocking steams提供消费 + 独立开生产和消费operators；
    
    2.平衡延迟和吞吐量
        缓冲被送往消费者①缓冲满②触发timeout；
        高吞吐：设置一个大的buffer size；
        低延迟：设置一个小的timeout值；
    
    3.Control Events
        checkpoint barriars
        watermarks
        iteration bariers

###3.3 容错
    exactly-once；
    检查点机制 + 部分回滚重新执行 + managed state + state backend；
    前提假设：数据源是持久的（不持久的可以用WAL）、可重放的(如消费Kafka队列数据)；
    检查点机制：Asynchronous Barrier Snapshotting（ABS）
        Barriers作为控制记录插入streams，一起向下流动，带有一个逻辑时间将stream划分为当前快照和以后将要快照的两部分；
        接受超过一个输入流的operator需要基于barrier对齐！operator一接收到某个输入流的barrierN就不能继续处理此数据流后序的数据，直到operator收到其余流的barrierN，否则会将snapshot n的数据和snapshot n+1的数据搞混；
                注：保证exactly-once降低了点效率，若收到一个barrierN后继续处理该流的数据，则barrierN以后的数据会影响到当前状态，故障恢复replay时就不保证exactly-once了，降级为at-least-once。
        收到所有输入数据流的barrier后，将operator状态快照，依赖state backend可由另一线程异步写入永久存储(如HDFS)；
    故障恢复时：
        找最近的检查点，将状态逆转到检查点的状态，replay。某一节点故障的情况下，可以仅仅部分回滚涉及故障节点的一条线，其余结点忽略replay消息。
    
    Chandy-Lanport算法，即Asynchronous Distributed Snapshotting（ADS）
        与ABS的区别是：接受超过一个输入流的operator，在收到第一个marker时就进行快照，并记录该channel的state为空集，然后向下游发送marker；收到第二个marker时，记录channel的state为快照时到收到该marker之间收到的消息......
        ADS不会阻塞，但要记录channel状态；ABS会阻塞，但省去了记录channel状态。
 
###3.4 Iterative Dataflows
    Head、Tail、Feedback
    Iteration Steps



##4.Steam Analytics on Top of Dataflows
###4.1 The Notion of Time
    3个时间概念：
        event-time(事件时间：不变的事件发生时间)
        ingestion-time(摄取时间：事件进入流处理系统的时间)
        processing-time(处理时间：消息在各节点被处理的机器系统时间)；
    
    "event-time + watermarks"机制：
        存在迟到、乱序问题，将low watermarks作为事件插入数据流中流转到下游operators；
        具有时间戳t的watermark表示断言小于等于该时间戳t的事件（在某种合理的概率上）都已到达；
        每个事件都会延迟一段时间才到达，而这些延迟差异会比较大，所以有些事件会比其他事件延迟更多，导致乱序。一种简单的方法是假设这些延迟不会超过某个最大值，Watermark(currentMaxTimestamp - maxOutOfOrderness)。Flink把这种策略称作“有界无序生成策略”（bounded-out-of-orderness）。当然也有很多更复杂的方式去生成 watermark，但是对于大多数应用来说，固定延迟的方式已经足够了；
        当前到达记录时间戳 > low watermark时，窗口关闭，触发窗口计算；
    
    processing-time：低延迟，但不保证确定性（取决于数据到达系统的速度、数据在operators之间流动的速度）；
    event-time：有一定延迟，但保证正确性（watermark机制）；
    "Because of that, event time programs are often combined with processing time operations"；

###4.2 Stateful Stream Processing
    状态对机器学习、图计算、用户session处理、窗口聚合等是关键。
    状态可以是一个计数器、sum、分类树、大稀疏矩阵、Stream Windows是状态算子（内存更新记录）；

###4.3 Stream Windows
    Time Window、Count Window；Tumbling Window、Sliding Window、Session Window：
        Tumbling/Sliding TimeWindow、Tumbling/Sliding CountWindow、SessionWindow
            .timeWindow(Time.minutes(1))
            .timeWindow(Time.minutes(1), Time.seconds(30))
            .countWindow(100)
            .countWindow(100, 10)
            .window(ProcessingTimeSessionWindows.withGap(Time.seconds(30)))
    
    Window API：
        ①WindowAssigner：决定某个元素被分配到哪个/哪些窗口中去；
            数据流源源不断地进入算子，每一个到达的元素都会被交给WindowAssigner。WindowAssigner会决定元素被放到哪个或哪些窗口（window），可能会创建新窗口。因为一个元素可以被放入多个窗口中，所以同时存在多个窗口是可能的。注意，Window本身只是一个ID标识符，其内部可能存储了一些元数据，如TimeWindow中有开始和结束时间，但是并不会存储窗口中的元素。窗口中的元素实际存储在Key/Value State中，key为Window，value为元素集合（或聚合值）。为了保证窗口的容错性，该实现依赖了Flink的State机制。
        ②Trigger：决定了一个窗口何时能够被计算或清除；
            每一个窗口都拥有一个属于自己的Trigger，Trigger上会有定时器，用来决定一个窗口何时能够被计算或清除。每当有元素加入到该窗口，或者之前注册的定时器超时了，那么Trigger都会被调用。Trigger的返回结果可以是 continue（不做任何操作），fire（处理窗口数据），purge（移除窗口和窗口中的数据），或者fire+purge。一个Trigger的调用结果只是fire的话，那么会计算窗口并保留窗口原样，也就是说窗口中的数据仍然保留不变，等待下次Trigger fire的时候再次执行计算。一个窗口可以被重复计算多次知道它被purge了。在purge之前，窗口会一直占用着内存。
        ③Evictor：在Trigger触发之后，在窗口被处理之前，Evictor（如果有Evictor的话）会用来剔除窗口中不需要的元素，相当于一个filter；
            当Trigger fire了，窗口中的元素集合就会交给Evictor（如果指定了的话）。Evictor主要用来遍历窗口中的元素列表，并决定最先进入窗口的多少个元素需要被移除。剩余的元素会交给用户指定的函数进行窗口的计算。如果没有Evictor的话，窗口中的所有元素会一起交给函数进行计算。
        [源码分析：]http://wuchong.me/blog/2016/05/25/flink-internals-window-mechanism/
        Flink对于一些聚合类的窗口计算（如sum,min）做了优化，因为聚合类的计算不需要将窗口中的所有数据都保存下来，只需要保存一个result值就可以了。每个进入窗口的元素都会执行一次聚合函数并修改result值。这样可以大大降低内存的消耗并提升性能。但是如果用户定义了Evictor，则不会启用对聚合窗口的优化，因为Evictor需要遍历窗口中的所有元素，必须要将窗口中所有元素都存下来。

###4.4 Asynchronous Stream Iterations
    机器学习、图计算



##5. Batch Analytics on Top of Dataflows
    批计算与流计算统一运行时，blocked data streams；
    必要时关闭阶段性快照，从最近的物化的流中replay；
    Blocking operators；
    a dedicated Dataset API；
    查询优化层；
###5.1 查询优化
    等价查询、代价模型

###5.2 内存管理
    内存管理、定制的序列化工具、缓存友好的数据结构和算法、堆外内存、JIT编译优化。
    显式的内存管理并用序列化方式存储对象。
    JVM 存在的几个问题：
        1.Java 对象存储密度低。一个只包含boolean属性的对象占用了16个字节内存：对象头占了8个，boolean属性占了1个，对齐填充占了7个。而实际上只需要一个bit（1/8字节）就够了。
        2.Full GC会极大地影响性能，尤其是为了处理更大数据而开了很大内存空间的JVM来说，GC会达到秒级甚至分钟级。
        3.OOM问题影响稳定性。OutOfMemoryError是分布式计算框架经常会遇到的问题，当JVM中所有对象大小超过分配给JVM的内存大小时，就会发生OutOfMemoryError错误，导致JVM崩溃，分布式框架的健壮性和性能都会受到影响。
                        
    Flink 积极的内存管理以及直接操作二进制数据有以下几点好处：
        1.减少GC压力。显而易见，因为所有常驻型数据都以二进制的形式存在 Flink 的MemoryManager中，这些MemorySegment一直呆在老年代而不会被GC回收。其他的数据对象基本上是由用户代码生成的短生命周期对象，这部分对象可以被 Minor GC 快速回收。只要用户不去创建大量类似缓存的常驻型对象，那么老年代的大小是不会变的，Major GC也就永远不会发生。从而有效地降低了垃圾回收的压力。另外，这里的内存块还可以是堆外内存，这可以使得 JVM 内存更小，从而加速垃圾回收。
        2.避免了OOM。所有的运行时数据结构和算法只能通过内存池申请内存，保证了其使用的内存大小是固定的，不会因为运行时数据结构和算法而发生OOM。在内存吃紧的情况下，算法（sort/join等）会高效地将一大批内存块写到磁盘，之后再读回来。因此，OutOfMemoryErrors可以有效地被避免。
        3.节省内存空间。Java对象在存储上有很多额外的消耗。如果只存储实际数据的二进制内容，就可以避免这部分消耗。
        4.高效的二进制操作&缓存友好的计算。二进制数据以定义好的格式存储，可以高效地比较与操作。另外，该二进制形式可以把相关的值，以及hash值，键值和指针等相邻地放进内存中。这使得数据结构可以对高速缓存更友好，可以从L1/L2/L3缓存获得性能的提升。
    Java生态圈实际上已经有不少出色的序列化库，包括Kryo、Apache Avro、Apache Thrift和 Google的ProtoBuf，然而Flink重新造了一套轮子以定制数据的二进制格式。
    这带来了三点重要的优势：其一，掌握了对序列化后的数据结构信息，使得二进制数据间的比较甚至于直接操作二进制数据成为可能；其二，Flink依据计划要执行的操作来提前优化序列化结构，极大地提高了性能；其三，Flink可以在作业执行之前确定对象的类型，并在序列化时利用这个信息进行优化。
    
    网络传输中的内存管理：
        1.TaskManager（TM）在启动时，会先初始化NetworkEnvironment对象，TM 中所有与网络相关的东西都由该类来管理（如Netty连接），其中就包括NetworkBufferPool。根据配置，Flink会在NetworkBufferPool中生成一定数量（默认2048）的内存块MemorySegment（关于 Flink 的内存管理，后续文章会详细谈到），内存块的总数量就代表了网络传输中所有可用的内存。NetworkEnvironment和NetworkBufferPool是Task之间共享的，每个TM只会实例化一个。
        2.Task 线程启动时，会向NetworkEnvironment注册，NetworkEnvironment会为Task 的InputGate（IG）和ResultPartition（RP）分别创建一个LocalBufferPool（缓冲池）并设置可申请的MemorySegment（内存块）数量。IG对应的缓冲池初始的内存块数量与 IG 中 InputChannel 数量一致，RP 对应的缓冲池初始的内存块数量与 RP 中的ResultSubpartition数量一致。不过，每当创建或销毁缓冲池时，NetworkBufferPool会计算剩余空闲的内存块数量，并平均分配给已创建的缓冲池。注意，这个过程只是指定了缓冲池所能使用的内存块数量，并没有真正分配内存块，只有当需要时才分配。为什么要动态地为缓冲池扩容呢？因为内存越多，意味着系统可以更轻松地应对瞬时压力（如GC），不会频繁地进入反压状态，所以我们要利用起那部分闲置的内存块。
        3.在 Task 线程执行过程中，当Netty接收端收到数据时，为了将Netty中的数据拷贝到 Task 中，InputChannel（实际是 RemoteInputChannel）会向其对应的缓冲池申请内存块（上图中的①）。如果缓冲池中也没有可用的内存块且已申请的数量还没到池子上限，则会向 NetworkBufferPool 申请内存块（上图中的②）并交给 InputChannel填上数据（上图中的③和④）。如果缓冲池已申请的数量达到上限了呢？或者 NetworkBufferPool 也没有可用内存块了呢？这时候，Task的Netty Channel会暂停读取，上游的发送端会立即响应停止发送，拓扑会进入反压状态。当Task线程写数据到 ResultPartition时，也会向缓冲池请求内存块，如果没有可用内存块时，会阻塞在请求内存块的地方，达到暂停写入的目的。
        4.当一个内存块被消费完成之后（在输入端是指内存块中的字节被反序列化成对象了，在输出端是指内存块中的字节写入到 Netty Channel 了），会调用 Buffer.recycle() 方法，会将内存块还给 LocalBufferPool （上图中的⑤）。如果LocalBufferPool中当前申请的数量超过了池子容量（由于上文提到的动态容量，由于新注册的Task导致该池子容量变小），则LocalBufferPool会将该内存块回收给 NetworkBufferPool（上图中的⑥）。如果没超过池子容量，则会继续留在池子中，减少反复申请的开销。

###5.3 批迭代
    1.批次迭代Bulk iteration
        Iteration Input、Step Function、Next Partial Solution、Iteration Result；
        迭代终止条件：
            达到最大迭代次数（Maximum number of iterations）；
            自定义的聚合收敛（Custom aggregator convergence）；
    
    2.差异性迭代Delta iteration



#State Management in Apache Flink
##1.Introduction
    缺少直接的计算状态抽象 --> 依赖外部数据库系统管理状态  --问题-->
        需要应用逻辑来协调计算与外部数据库系统；
        外部存储会成为瓶颈；
        扩展操作困难；
    
    Spark牺牲了编程模型透明和处理延迟，使用以批处理为中心的应用逻辑；
    还有其它一些建立在笨重的每条记录交易处理之上；
    Flink整合了状态管理与计算；
        基于分布式快照的一致性状态管理
            modular state backend
            failure recovery
            reconfiguration
    
    有些快照机制：全局同步，备份时停止计算，保存了不需要的状态等；
    Flink的分布式快照机制久经考验（state backend ==> 可由另外一线程异步snapshot）；


##2.State
###2.1 Managed State
    1.Keyed-State
        基于KeyedStream
        Key-Groups是redistribute的最小单位（权衡了重分配时间和重分配所需元数据）
    2.Operator-State
        Non-Keyed，与Operator绑定，整个Oprerator只对应一个state
        将状态存入ListState，遍历ListState来分配
    state的数据结构：
        ValueState、ListState、ReduceState、MapState
    
    State Backend状态后端
    
    有了状态后端的支持，使用copy-on-write技术异步快照到持久存储中。


##Exactly Once
    1.内部exactly-once:
        checkpoint机制
    2.外部exactly-once（端到端end-to-end）:        
        ①Idempotent Sinks
        ②Transactional Sinks
            事务sinks两阶段提交：
                pre-commit：
                每个进行snapshot的operator成功snapshot后,都会向JobManager发送ACK callback；当sink完成snapshot后, 向JobManager发送ACK的同时向kafka进行pre-commit；
                commit：
                当JobManager接收到所有算子的ACK后,就会通知所有的算子这次checkpoint已经完成。Sink接收到这个通知后, 就向kafka进行commit,正式把数据写入到kafka；
