[TOC]
#1.MapReduce
##1.概述
###1.1 分布式并行编程
    过去：单指令、单数据流顺序执行 --> 提高CPU性能
    现在：摩尔定律失效,分布式并行编程 --> 向集群中增加新的计算节点

###1.2 MapReduce模型简介
    ①前提条件：待处理数据集可以分解成许多小的数据集，且每个小数据集可以完全并行处理。
    ②分布式文件系统中的数据切分成多个独立的小数据块，被多个Map任务并行处理，Map输出的中间结果作为Reduce的输入。
    ③“移动计算比移动数据更经济”
    ④Master/Slave架构，Master上运行JobTracker，Slave上运行TaskTracker。

###1.3 Map和Reduce函数
    (受函数式编程启发)编程容易，因为程序员只需关注Map和Reduce函数，而无需关注分布式存储、工作调度、负载均衡、容错处理、网络通信等问题。
    Map        <k1,v1>           List(<k2,v2>)
    Reduce     <k2,List(v2)>     <k3,v3>
    Map任务的每个键都会经过Hash函数计算，并根据哈希结果将该键值对输入相应的Reduce任务来处理。


##2.MapReduce的工作流程
###2.1 工作流程概述
    核心思想：“分而治之”。将一大数据集拆分成多个小数据块在多台机器上并行处理。移动计算，计算和数据可以放在一起运行，避免额外的数据传输开销。

###2.2 MapReduce各执行阶段
    ①InputFormat预处理，如输入格式是否符合输入定义；将输入文件切分为逻辑上的M个InputSplit(位置+长度)；
    ②RecordReader（RR）根据InputSplit信息加载数据并转换为适合Map任务读取的键值对，输入给N个Map任务；
    ③N个Map任务根据用户自定义映射规则，输出<key,value>中间结果；
    ④Shuffle：对Map的输出进行分区、排序（Sort）、合并（Combine）、归并（Merge），得到<key,value-list>形式的中间结果，再交给相应的Reduce处理；
    ⑤R个Reduce执行用户定义逻辑，输出结果给OutputFormat模块；
    ⑥OutputFormat模块验证输出目录是否已经存在以及输出结果是否符合配置文件中的配置类型，都满足，则输出Reduce结果到分布式文件系统。

###2.3 Shuffle过程详解
    Map端
        ①输入数据执行Map任务；
        ②输出（序列化）写入缓存；
        ③溢写(分区、排序、合并(减少网络传输数据量))；
        ④归并多个溢写文件（溢写文件数量超过设置的参数时触发）；
        JobTracker监测Map任务的执行，检测到一个Map任务完成就会立即通知相应的Reduce来领取数据。

    Reduce端
        ①领取数据；
        ②归并溢写文件数据(默认对键值对排序，一次归并的文件数量由参数决定)；
        ③把数据输入Reduce任务（多轮归并后得到若干个大文件不会继续归并成一个，直接输入Reduce任务以减小磁盘读写开销）；


##3.容错
###3.1 工作节点故障
    已完成的Map任务需要重新执行，因为输出的中间结果存储在故障机器的磁盘上不能被访问了；
    已完成的Reduce任务不需要重新执行，因为输出已存储在一个全局文件系统中。

###3.2 主节点故障
    从检查点恢复。


##4.Locality
    GFS将输入文件切分成64MB大小的块，并且将每个块的多份副本存储在不同的机器上。
    MapReduce的master获取所有输入文件的位置信息，然后将Map任务调度到有相应输入文件副本的机器上（或最近的机器），以节省网络带宽。


##5.任务粒度
    M个Map任务、R个Reduce任务，比worker machine数量多得多。
    
    实际中M和R有实用范围，因为master要做O(M+R)个调度决策，且在内存中保存O(M*R)个状态。
        
    且R通常受限于用户，因为各个Reduce任务要输出到独立的各个输出文件中。实际中，我们会选择M使得每个输入文件大概16MB到64MB（使Locality达到最优）；选择R为worker machine的一个小的倍数。
    (M=200000,R=5000,worker machine=2000)


##6.备份任务机制
    一个由于某些原因（CPU，内存，本地磁盘，网络带宽）而拖后腿（执行过长的时间）的任务，拖慢了整个MapReduce操作。
    措施：当MapReduce运行接近尾声时，存在一部分worker仍在执行剩余任务时，将这一部分任务调度给其他机器备份执行。原任务和备份任务中但凡有一个完成，该任务即标记为已完成。



##7.改善
###7.1 Partitioning Function
    默认分区函数使用Hash（如hash(key) mod R），划分均衡。
    也可自定义。

###7.2 Ordering Guarantees
    确保在每个分区中，中间键值对按key升序排序。
    这样，确保每个分区产生一个排好序的输出文件；对输出文件的按key随机查找方便；用户使用也方便。

###7.3 Combiner Function
    在某些情况下，由各个Map任务产生的中间键会有大量的重复。
    如WordCount中Map任务会出现大量<the,1>，且被通过网络送往同一个Reduce任务。在网络传输之前就在Shuffle中做一些合并，可以大大减小网络传输开销。

###7.4 Input Output Types
    MapReduce库提供了对输入/输出文件多种格式的支持。（例如，"text"格式的输入将每一行作为键值对：key是文件内的偏移，value是该行的内容。）
    每一个输入/输出格式的实现都知道如何将自己进行合理的划分从而能让不同的Map任务进行处理（例如，text模式就知道将区域划分到以行为边界）。

###7.5 其它
    辅助额外输出文件、跳过坏的记录、本地执行、状态信息（Http Server状态页）、Counter








#2.Map-Reduce-Merge
##0.Abstract
    MapReduce不直接支持处理多重关联的混合数据集（关系型操作如join），而处理关系型数据又是一种普遍需求；
    Map-Reduce-Merge模型为MapReduce添加了一个Merge语义，可以利用map和reduce模块高效地归并已经分区和排序/哈希的数据；
    且该新模型可以表达关系型代数算子并实现了几种join算法；


##1.Introduction
    Map-Reduce-Merge通过附加的MapReduce步骤，来实现多种混合数据集的关系代数原语(如join)；
    关系型算子可以由map、reduce和merge组合而成，可以实现多个并行版本的join算法：sort-merge、hash、block nested-loop；


##2.MapReduce
###2.1 Features and Principles
    ①廉价不可靠商用硬件，而非SMP或MMP；
    ②高可扩展RAIN(Redundant Array of Independent(and Inexpensive) Nodes)集群，而非基于RAID的SAN或NAS；
    ③容错但易于管理，不想DBMS一样复杂的备份、恢复；
    ④简化且受限但强大，只关注map、reduce；
    ⑤高度并行但抽象；
    ⑥高吞吐，移动计算到数据节点；
    ⑦共享磁盘存储但不共享计算；
    ⑧面向集合的Key-Value对，文件抽象；
    ⑨函数式编程原语；
    ⑩分布式分区/排序框架；
    ⑪可应用到通用数据处理任务；

###2.2 Homogenization
    key + datasource；
    A final map/reduce task can then apply to all the homogenized datasets combined. 
    Data entries from different datasets with the same key value will be grouped in the same reduce partition.
    User-defined logic can extract data-sources from values to identify their origins, then the entries from different sources can be merged.
    只能做等值连接；


##3.Map-Reduce-Merge
    map:      (k1, v1)α                   →  [(k2, v2)]α
    reduce:   (k2, [v2])α                 →  (k2, [v3])α
    merge:    ((k2, [v3])α, (k3, [v4])β)  →  [(k4, v5)]γ
    其中α、β是dataset的lineage；
    依旧是原生的map、reduce；
    唯一的变化是：
        增加了lineage；
        (k2,[v3])而非仅仅[v3]；
    做出变化的原因是merge阶段需要输入数据按key组织(partitioned, sorted/hashed)；

###3.2 Implementation
    Merge Components：
        partition selector；
        processor function；
        merge function；
        configurable iterator；
    partition selector：
        通过merger number决定merger从哪些reducer中取数据；
        (reducer number用于决定从哪些mapper中取数据，mapper number决定取哪些输入文件的split)
        当前merger number + 两个reducer numbers集合，用户自定义的逻辑会从集合中删除不相关的reducers，只有留在集合中的reducers才会被读取并在merger中merge；
        partitionSelector.select(mergerNumber, leftReducerNumbers, rightReducerNumbers);
    processor：
        用户自定义的用于处理一个source数据的函数；
    merger:
        用户自定义处理逻辑，mapper处理一对key/value、reducer处理key-grouped value集合、merger处理两对key/values；
    configurable iterator:
        logical iterator：
            mapper：input flie split；
            reducer：a merge-sorted stream；
            merger：two logical iterators；
        a user-configurable module(iterator-manager) controls the movement of these configurable iterators；
        nested-loop、sort-merge、hash join；


##4.Applications to Relational Data Processing
###4.1 Map-Reduce-Merge Implementations of Relational Operators
    Map-Reduce-Merge是关系完备的。
    Projection：
        mapper：(k, v) -> (k', v')；
    Aggregation：
        sort-by-key、 group-by-key保证了输入到reducer是一系列(k, [v])集合，reducer可以在这个grouped value list上调用aggregate函数；
    Generalized Selection：
        如果选择条件发生在一个数据源 => 用mapper实现选择(filter)；
        如果选择条件发生在aggregate或一组values => 用reducer实现选择(filter)；
        如果选择条件发生在多个数据源的属性或aggregate => 用merger实现选择(filter)；
    Joins：
        见4.2；
    Set Union：
        sorted and grouped，
        reducer中去重简单，merger会收到来自多个reducers的数据，需要再次去重；
    Set Intersection：
        merger iterate每个输入，并产生共同的输出；
    Set Difference：
        merger iterate每个输入，并产生difference的输出；
    Cartesian Product：
        可以用nested-loop实现；
    Rename：
        简单；

###4.2 Map-Reduce-Merge Implementations of Relational Join Algorithms
    ①Block Nested-Loop Join：
        Map：使用hash partitioner将记录分到hashed buckets中(一个bucket对应一个reducer)；
        Reduce：group and aggregate partitions using a hash table；
        Merge：(利用内存cache)block nested-loop；
    ②Sort-Merge Join：
        Map：使用range partitioner将记录分到有序的buckets中(一个bucket对应一个reducer)；
        Reduce：使用外部排序(priority queue)归并成一个sorted set；
        Merge：已排序，只要merge；
    ③Hash Join：
        Map：使用hash partitioner将记录分到hashed buckets中(一个bucket对应一个reducer)；
        Reduce：group and aggregate partitions using a hash table；
        Merge：a build and a probe；


##5.Optimization
###5.1 Optimal Reduce-Merge Connections
    减少Reducers和Mergers之间的网络连接；
    partition selector，R^2 + R  ==>  2R (没懂)

###5.2 Combining Phases
    ReduceMap，MergeMap；
    ReduceMerge；
    ReduceMergeMap；
    组合语句，包含很多磁盘读写传递；








#3.Twister：a runtime for Iterative MapReduce
##0.Abstract
* 支持高效的迭代MapReduce计算；


##1.Introduction
* 传统的并行应用使用Message Passing（如MPI、PVM），可以提供的一套复杂的通信拓扑和严格的同步构成；相反的，MapReduce等高级编程模型支持简单的通信拓扑和同步构成；
* MapReduce在某些应用中表现突出，但是在聚类、机器学习、计算机视觉等的迭代计算中表现不佳，每一步迭代都重复加载、处理所有数据；
* Twister：
    基于发布/订阅消息机制进行通信和数据传输；
    为MapReduce扩展了broadcast和scatter类型数据传输；


##2.MapReduce
* 回忆MapReduce，此处不再赘述；


##3.Iterative MapReduce With Twister
###3.1 Static vs. Variable Data
* 静态数据在计算中保持不变，可变数据是每次迭代计算的结果且是下一次迭代的输入；

###3.2 Long Running Map/Reduce Tasks
* 在每次迭代中不reload静态数据；
* 而在Hadoop、DryadLINQ中每次迭代都要重新创建map/reudce task以及加载静态数据(代价很高)；

###3.3 Granularity of Tasks
* 通过配置的map task，一个map task可以访问多个数据文件blocks，以减少中间结果数据量；
* Hadoop中的Combiner也是同样的目的；

###3.4 Side-effect-free Programming
* 只存储静态配置，不存储临时状态；（没懂什么意思，副作用？？？）

###3.5 Combine Operation
* Combine操作：可以通过Combine操作来产生一个所有reduce outputs的collective output；

###3.6 Programming Extensions
* MapReduceBCast(Value value)：发送一个Value (MapReduce中为(key, value) pairs(可以是参数、资源名、数据...))对给所有map tasks；
* configureMaps(Value[] values)、configureReduce(Value[] values)


##4.Twister Architecture
![](http://ww1.sinaimg.cn/mw690/006aTs3igy1fzcv8euxqvj30r60k3qb6.jpg)
* in-memory；
* 从worker nodes本地磁盘加载数据；
* 在worker nodes的分布式内存中处理中间数据；
* 发布/订阅消息基础结构通信：
    * ①发送/接收控制事件；
    * ②客户端驱动发送数据给守护进程；
    * ③map和reduce任务之间的数据传输；
    * ④将reduce任务的输出发送回客户端驱动来唤醒combine操作；
* 架构组成：
    * ①客户端driver，驱动整个MapReduce计算；
    * ②运行在worker node上的守护进程，接收命令和数据；
    * ③broker network（发布/订阅消息结构）；
    
###4.1 Handling Input and Output Data
* file based input format;用户需要提前将大量数据分成许多的小文件；
* 从partition file中读取分布在worker nodes的输入文件的元数据信息(file_id, node_id, file_path, replication_no)；
*  提供类似于GFS、HDFS部分功能的文件操作工具；
*  也可以通过broker network将输入数据发送给map tasks，对大量的输入数据很低效，但是对于发送一些小量的可变数据（如参数、矩阵的行数、集群中心等）很有用；

###4.2 Handling Intermediate Data
* map tasks输出的中间结果通过broker network直接push给相应的reduce tasks，缓存在内存中知道reduce计算开始；
* 若中间结果集太大，可扩展Twister运行时将reduce的输入存储在本地磁盘中，而不是内存缓冲中；

###4.3 Use of Pub/Sub Messaging
* NaradaBrokering系统配置一个broker network；

###4.4 Scheduling Tasks
* cache、static sheduling

###4.5 Fault Tolerance
* 存储迭代间的计算状态，失败时可以回滚状态；
* 前提条件：①master node不failure；②通信架构独立容错；③数据在计算节点之间有备份；
* combine操作是迭代MapReduce计算中隐含的全局barrier！可以简化容错的状态，只保存配置信息(static data)来配置map/reduce tasks以及主程序直接发送给map tasks的输入数据；








#4.HaLoop：Efficient Iterative Data Processing
##0.Abstract
* MapReduce和Dryad等缺少对迭代计算的内建支持；
* loop-aware task scheduler、caching mechanism；


##1.Introduction
* MapReduce不天生支持迭代计算，需要编程者发动多个MapReduce作业来实现迭代计算，且存在以下两个主要问题：
    * 问题①在迭代之间许多数据是保持不变的，却仍需要在每个迭代中重新加载、处理这些没变的数据，浪费了I/O、网络带宽和CPU资源；
    * 问题②迭代终止条件可能涉及探测何时到达了fixpoint，这个条件可能需要给每次迭代添加额外的MapReduce作业，又会导致调度额外的作业、从磁盘读取额外的数据、在网络中移动数据；
* HaLoop：
    * ①Loop-Aware，co-locating不同迭代间处理相同数据的tasks；
    * ②在第一次迭代中cache不变的数据，以在后面的迭代中重用；
    * ③cache reducer输出，以检查是否到达fixpoint；


##2.HaLoop Overview
###2.1 Architecture
![](http://ww1.sinaimg.cn/mw690/006aTs3igy1fzd3c7fkqaj30v00qun2g.jpg)
* 像Hadoop一样依赖于一个分布式文件系统来存储输入/输出数据；
* 像Hadoop一样Master + Slaves，Client提交作业给Master，Master schedules 并行tasks到Slaves上；
* 改变：
    * ①暴露一个新的编程接口以简化迭代MapReduce程序的表达；
    * ②在Master中添加了loop control模块，重复启动新的map-reduce steps，直到达到终止条件；
    * ③为迭代应用设计的新的scheduler，可以更好地利用数据局部性；
    * ④在Slave Nodes上缓存并索引应用数据；

###2.2 Programming Model
* R[i+1] = R[0] ∪ (R[i] ⋈ L)
* fixpoint:
    * 默认:连续两次迭代之间相等;
    * approximate fixpoint(useful for convergence condition)：
        * 连续两次迭代之间的差别小于用户定义的阈值；
        * 达到了最大迭代次数；
* 写一个HaLoop程序：
    * 循环体(map-reduce pairs)；
    * 终止条件；
    * loop-invariant数据；
* 只提供API，没有高级的声明查询语言，希望别的高级语言可以建立在此基础之上；
    * Map、Reduce、AddMap、AddReduce；
    * SetFixedPointThreshold、ResultDistance、SetMaxNumOfIterations；
    * SetIterationInput、AddStepInput、AddInvariantTable；


##3.Loop-Aware Task Scheduling
###3.1 Inter-Iteration Locality
* 将不同迭代中处理相同数据的map/reduce tasks放置在相同的物理机器上；
* inter-iteration locality：相邻迭代之间输入数据相同；
* 要求：reduce任务数量在迭代中要保持固定，这样mappers输出分配到reducers的Hash函数可以保持不变；

###3.2 Scheduling Algorithm
* 跟踪存储在物理机器上数据分区，利用这些信息调度Inter-Iteration Locality的后续map/reduce任务；
* Master保存所有slaves节点到其前一次迭代处理数据的映射关系；
![](http://ww1.sinaimg.cn/mw690/006aTs3igy1fzen9wsbuzj30t20qa0y4.jpg)


##4.Caching and Indexing
###4.1 Reducer Input Cache


###4.2 Reducer Output Cache


###4.3 Mapper Input Cache


###4.4 Cache Reloading











#5.MapReduce Online
##0.Abstract
    批处理MapReduce：
        将map和reduce任务的整个输出materialize，在大规模集群部署下可以更方便地实现fault tolerance；
    HOP(Hadoop Online Prototype)：
        支持持续查询，可以用于事件监控和流处理；
        数据在算子之间是pipelined的；


##1.Introduction
    批处理MapReduce：
        data-centric；
        materialization；
    Hadoop Online Prototype：
        中间数据在算子间pipelined；
    Pipelined MapReduce's benefits：
        ①mappers产生的数据一到达就可以被reducers处理，使用online aggregation技术在执行过程中产生和refine最终结果；
        ②支持持续
        查询，可用于事件监控和流处理；
        ③数据发送给下游算子更迅速，加快处理速度；
    Pipelined MapReduce's challenges：
        ①新的fault tolerance；
        ②pipelines之间的communication；
        ③生产者和消费者协同调度；
