[TOC]
#Storm
##1.流计算概述
###1.1 流数据的特征
    数据快速持续到达，无穷无尽；
    数据来源众多，格式复杂；
    数据量大，但不十分关注存储，一旦数据经过处理，要么被丢弃，要么被归档存储；
    注重数据的整体价值，不过分关注个别数据；
    数据顺序颠倒或不完整，系统无法控制将要处理的新到达的数据元素的顺序；

###1.2 MapReduce框架为何不适合处理流数据？
    流数据不适合采用批量计算，因为流数据不适合用传统的关系型模型建模，不能把源源不断的数据保存到数据库中，流数据被处理后，一部分进入数据库成为静态数据，其它部分则直接被丢弃。
        
    批量任务处理在延迟方面无法满足流计算的实时响应需求。MapReduce在多台机器上并行运行MapReduce任务，最后对结果进行汇总输出，有时完成一个任务需要多轮的迭代。
    ==> 将基于MapReduce的批量处理转化为小批量处理。带来如下问题：
        ①切分成小片虽可降低延迟，但增加了任务处理的附加开销，且还要处理片间的依赖关系；
        ②需对MapReduce进行改造以支持流式处理，Reduce结果不能直接输出，而是保存在内存中，会增加MapReduce框架复杂度，导致系统难以维护和扩展；
        ③降低了用户程序的可伸缩性，用户必须使用MapReduce接口来定义流式作业；
    
    MapReduce是专门面向静态数据的批量处理的，内部实现机制做了高度优化，不适合用于处理持续到达的动态数据。


##2.流计算的处理流程
    数据实时采集 --> 数据实时计算 --> 实时查询服务
    
    传统的数据处理流程，需要用户主动发出查询；
    而在流处理流程中，实时查询服务可以不断更新结果，并将用户所需的结果实时推送给用户。


##3.流计算的应用
    实时分析（推荐系统实时推荐）
    实时交通、实时销量、流量统计
    监控预警、金融系统


##4.开源流计算框架Storm
    可扩展：并行运行在分布式集群中
    异常健壮：集群易管理，可轮流重启节点
    容错性好：自动进行故障节点的重启和任务重新分配
    可靠的消息处理：Storm保证每个消息都能完整处理
###4.1 Storm设计思想
    1.Streams
        Streams是流数据的抽象，流数据是一个无限的Tuple序列。
        Tuple序列以分布式并行创建和处理。
    
    2.Spouts
        Spouts是每个Stream的源头的抽象。
        Spouts会从外部读取流数据并持续发出Tuple。
    
    3.Bolts
        Bolts是Streams的状态转换过程的抽象。
        Bolts处理到来的Tuple，并将处理后的Tuple作为新的Streams发送给next Bolts。
    
    4.Topology
        Topology是Spouts和Bolts组成的网络的抽象。
        Topology是Storm中最高层次的抽象概念，可以被提交到Storm集群运行。一个Topology就是一个流转换图（可以有环），图中的结点为Spout或Bolt（Computation），图中的边表示Bolt订阅了哪个Stream（Flow）,当Spout或Bolt发送元组时，它会把元组发送到每个订阅了该Stream的Bolt上处理。
    
    5.Stream Groupings
        Stream Grouping用于告知Topology如何在两个组件间（如Spout和Bolt之间或不同的Bolts之间）进行Tuple的传送。
        6种分组方式：
            Shuffle Grouping：随机分组，随机分发Tuple；
            Fields Grouping：按字段分组，具有相同值的Tuple分发到对应的Bolt；
            All Grouping：广播分发，每个Tuple都会被分发到所有Bolt中；
            Global Grouping：全局分组，Tuple只会分发给一个Bolt；
            Non Grouping：不分组，与随机分组效果类似；
            Direct Grouping：直接分组，由Tuple的生产者来定义接受者。

###4.2 Storm框架设计
    分布式集群运行"Topology"，持续处理消息(直到人为终止)。
    
    "Master-Worker"主从架构：
        Master节点运行"Nimbus"后台程序。分发代码；为Worker分配任务；监测故障；
        Worker节点运行"Supervisor"后台程序。负责监听Master分配的任务以启动/停止Worker进程(一个supervisor可以启动多个Worker，一个Worker一个slot)；
                                   Supervisor          Workers        Executors
                  Zookeeper        Supervisor          Workers        Executors
    Nimbus  <==>  Zookeeper  <==>  Supervisor   <==>   Workers  <==>  Executors
                  Zookeeper        Supervisor          Workers        Executors
                                   Supervisor          Workers        Executors
    
        worker process = worker receive thread + worker send thread
        executor = user logic thread + executor send thread
    
    采用Zookeeper分布式协调组件(文件系统+通知机制)：
        ①Nimbus与Supervisor进行通信(分配任务和心跳)，同步；
        ②Supervisor和Worker进行通信(分配任务和心跳)；
        ③Nimbus高可用(HA机制)；
        Nimbus和Supervisor后台进程都是快速失败(Fail-fast)和无状态(Stateless)的，Master节点不和Worker节点直接通信，而是借助Zookeeper将状态存放在Zookeeper或本地磁盘中，以便节点故障时快速恢复。(==>极其稳定)
    
    工作流程：
        ①client提交Topology到Storm集群中；
        ②Nimbus读取代码和配置，将分配给Supervisor的任务写入Zookeeper；
        ③Supervisor从Zookeeper中获取所分配的任务，并启动Worker进程；
        ④Worker进程启动一个或多个Executor线程执行具体的任务。

###4.3 可靠数据处理
    1.at-least-once
        输入Topology的每个Tuple至少被处理一次。
        实现方案：
            ①设置“acker" Bolt跟踪每个Spout发出的Tuple。
            ②为每个Tuple附一个64位的“messageid”①在其流入Spout时或者②在Tuple处理时所产生/完成的新的Tuple，将message id表保存在provenance tree中。
            当一个Tuple流出Topology时，用backflow机制找到其Spout。
    2.at-most-once
        每个Tuple要么被处理一次，要么作为failure被丢弃。
        when acking mechanism is disabled。
            
    
    acker bolt
    Tuple-at-a-time
    spout messageid
    tuple树处理成功/失败（只要任一节点失败则为失败）
    anchor(oldTuple，newTuple) 锚定
    XOR
        lineage跟踪在处理很多Tuple、很复杂的时候，维持跟踪将耗费acker的内存；
        每次产生或处理完成一个Tuple时，将①原始Tuple message id + ②XORed message ids + ③timeout parameter发送给acker bolt；
        当一个Tuple离开Topology时，若其XORed message ids = 0，则告诉Spout处理成功，若不等于0或者超过timeout还未收到ack和fail，则认为处理失败，告诉Spout重新发送该Tuple重新处理。




#Heron
##1.Introduction
    性能提升
    更少的资源消耗
    debug-ability
    可扩展
    易管理
    向下兼容Storm and Summingbird APIs


##2.Motivation for Heron
###2.1 Storm Worker Architecture
    worker设计十分复杂。
    ①executor线程调度 + 线程内部tasks调度  ==>  难以确定tasks何时被调度；
    ②每个worker可以运行不同的tasks  ==>  难以独立每个task的行为、性能和独自的资源占用  ==>  重启Topology解决纷争  ==>  任务会被重新调度到与其他任务一起运行，难以追溯到原始问题；
    ③一个task的未处理的异常会拖累整个worker，导致其它tasks也被杀死；
    ④各个tasks的日志被写入同一个文件  ==>  日志杂乱难以定位某个task的错误和异常；
    ⑤不同的tasks  ==>  垃圾回收难以实现；
    ⑥Storm认为每个worker都是对等的  ==>  资源过度分配；
    ⑦worker的receive和send线程 + executor的userlogic和send线程  ==>  过载和队列争夺，Queue是瓶颈、内存阻塞；

###2.2 Storm Nimbus
    职能过多，成为瓶颈。
    任务调度、监测、分发JARs、指标报告、管理Topology counters。
    ①资源调度问题（没有资源隔离）；
    ② 基于Zookeeper的Nimbus和Supervisor之间的心跳机制，Zookeeper成为瓶颈，于是使用“heartbeat” daemons，需要独立监视主机和心跳守护进程；
    ③单点故障；

###2.3 Lack of Backpressure
    丢弃tuple的fail-fast机制会带来如下问题：
        如果关闭ack机制，则Tuple丢失无法量化
        上流工作作废；
        系统行为难以预测；

###2.4 Efficiency
    ①tuple树中一个节点失败，重放整个tuple树；
    ②占用大量内存的Topology的worker垃圾回收慢，导致高延迟、高tuple失败率；
    ③队列瓶颈；


##3.Heron
