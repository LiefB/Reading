[TOC]
#Pregel
##0.Abstract
    背景：社交网络，数十亿顶点、数万亿边；
    以顶点为中心的策略：
        程序使用一系列的迭代过程来表达，在每一次迭代中，每个顶点会接收来自上一次迭代的信息，并发送信息给其它顶点，同时可能修改其自身状态以及以它为顶点的出边的状态，或改变整个图的拓扑结构。


##1.Introduction
    路径问题：最短路径、可达路径；匹配问题：结构匹配、语义匹配；聚类算法；PageRank算法；
    
    目前还不存在一种在大规模分布式环境下，可以基于各种图表示方法来实现任意图算法的，可扩展的通用系统；
    
    处理大规模图的算法：
        ①为特定图应用定制相应的分布式实现；不通用；
        ②基于现有的分布式计算平台(如MapReduce)；不理想；
        ③使用单机的图算法库；不能大规模；
        ④使用已有的并行图计算系统(Parallel BGL、CGMgraph)；无容错等分布式系统必要特性；
    
    Pregel：大规模、容错、API有效表达各种图算法；
        inspired by BSP(Bulk Synchronous Parallel) model 整体同步并行计算模型
        vertex-centric  以顶点为中心
        superstep
        synchronicity  同步
    
    BSP模型：
        1.Processors指的是并行计算进程，它对应到集群中的多个结点，每个结点可以有多个Processor；
        2.LocalComputation就是单个Processor的计算，每个Processor都会切分一些结点作计算；
        3.Communication指的是Processor之间的通讯。接触的图计算往往需要做些递归或是使用全局变量，在BSP模型中，对图结点的访问分布到了不同的Processor中，并且往往哪怕是关系紧密具有局部聚类特点的结点也未必会分布到同个Processor或同一个集群结点上，所有需要用到的数据都需要通过Processor之间的消息传递来实现同步；
        4.Barrier Synchronization又叫障碍同步或栅栏同步。每一次同步也是一个超步的完成和下一个超步的开始；5.Superstep超步，这是BSP的一次计算迭代，拿图的广度优先遍历来举例，从起始结点每往前步进一层对应一个超步。
        6.程序该什么时候结束呢？这个其实是程序自己控制，一个作业可以选出一个Proceessor作为Master，每个Processor每完成一个Superstep都向Master反馈完成情况，Master在N个Superstep之后发现所有Processor都没有计算可做了，便通知所有Processor结束并退出任务。



##2.Model of Computation
    overall：
        读取输入初始化该图，当图被初始化好后，运行一系列的superstep直到整个计算结束，这些superstep之间通过一些全局的同步点分隔，输出结果结束计算。
    计算的输入是一个有向图：
        顶点：有一个string顶点描述符
        边：与源顶点、目标顶点关联
    迭代计算：
        在每个superstep中，顶点的计算都是并行的，每个顶点执行相同的用于表达给定算法逻辑的用户自定义函数。每个顶点可以修改其自身及其出边的状态，接收前一个superstep(S-1)中发送给它的消息，并发送消息给其他顶点(这些消息将会在下一个superstep中被接收)，甚至是修改整个图的拓扑结构。在这种计算模式中edge并不是一等公民,因为没有相应的计算运行在边上。
    算法的终止：
        算法的终止取决于是否所有的顶点都已经“vote”标识其自身已经达到“halt”状态了。在第0个superstep，所有顶点都处于active状态，所有的active顶点都会参与所有对应superstep中的计算。顶点通过将其自身的status设置成“halt”来表示它已经不再active。这就表示该顶点没有进一步的计算需要执行，除非被再次被外部触发，而Pregel框架将不会在接下来的superstep中执行该顶点，除非该顶点收到其它顶点传送的消息。如果顶点接收到消息被唤醒进入active状态，那么在随后的计算中该顶点必须显式的deactive。整个计算在所有顶点都达到“inactive”状态，并且没有message在传送的时候宣告结束。
    计算的输出是所有顶点输出的集合：
        通常是一个与输出同构的有向图，但不是必须，因为计算过程中可以添加或删除顶点和边。比如聚类算法可能是从一个大图中生成的非连通顶点组成的小集合。
    
    异步批量传递消息
    superstep


##3.The C++ API
    继承Vertex类。
    template <typename VertexValue, typename EdgeValue, typename MessageValue>
    class Vertex {
        public:
        //定义的虚函数，Computer（）；用户的处理逻辑在这实现
        virtual void Compute(MessageIterator* msgs) = 0;
        //参数顶点ID
        const string& vertex_id() const;
        //记录执行的超步数
        int64 superstep() const;
        //获取顶点关联的值
        const VertexValue& GetValue();
        VertexValue* MutableValue();
        //出射边迭代器，获取所有的出射边
        OutEdgeIterator GetOutEdgeIterator();
        //发送消息
        void SendMessageTo(const string& dest_vertex, const MessageValue& message);
        //设为停机
        void VoteToHalt();
    };

###3.1 Message Passing
    ①消息传递有足够的表达能力；②异步、批量传递消息，缓解远程读取的延迟；
    顶点之间的通信是直接通过发送消息，每条消息都包含了消息值和目标顶点的名称。消息值的数据类型是由用户通过Vertex类的模版参数来指定。
    在一个superstep中，一个顶点可以发送任意多的消息。当顶点V的Compute()方法在superstep S+1中被调用时，所有在superstep S中发送给顶点V的消息都可以通过一个迭代器来访问到。在该迭代器中并不保证消息的顺序，但是可以保证消息一定会被传送并且不会重复。
    dest_vertex并不一定是顶点V的相邻顶点。一个顶点可以从之前收到的消息中获取到其非相邻顶点的标识符，或者顶点标识符可以隐式的得到。
    当一个消息的目标顶点不存在时，便执行用户自定义的handlers。可以创建该不存在的顶点或从源顶点中删除这条边。

###3.2 Combiners
    Combine减少消息数量来减少网络传输和缓存开销。与MapReduce的Combiner同曲同工。
    默认关闭，要开启Combiner的功能，需要继承Combiner类，覆写其virtual函数Combine()。

###3.3 Aggregators
    可以做统计，如sum aggregate统计每个顶点的出度；
    可以做全局协调，如Compute()函数的一些逻辑分支可能在某些superstep中执行，直到and aggregator表明所有顶点都满足了某条件，之后执行另外的逻辑分支直到结束；又比如一个作用在顶点ID之上的min和max的aggregator，可以用来选定某顶点在整个计算过程中扮演某种角色等；
    还可以用来实现一个△-stepping最短路径算法所需要的分布式优先队列；
    要定义一个新的aggregator，用户需要继承预定义的Aggregator类，并定义在第一次接收到输入值后如何初始化，以及如何将接收到的多个值最后reduce成一个值。Aggregator操作也应该满足交换律和结合律；
    默认情况下，一个aggregator仅仅会对来自同一个superstep的输入进行聚合，但是有时也可能需要定义一个sticky aggregator，它可以从所有的supersteps中接收数据。这是非常有用的，比如要维护全局的边条数，那么就仅仅在增加和删除边的时候才调整这个值了；

###3.4 Topology Mutations
    有一些图算法可能需要改变图的整个拓扑结构。比如一个聚类算法，可能会将每个聚类替换成一个单一顶点，又比如一个最小生成树算法会删除所有除了组成树的边之外的其他边。正如用户可以在自定义的Compute()函数能发送消息，同样可以产生在图中增添和删除边或顶点的请求；
    同一个superstep中可能产生冲突的请求(如两个请求都要增加一个顶点V，但初始值不一样)，两个机制保证确定性：局部有序和handlers；
    局部有序：
        先删边，后删点；
        先加点，后加边；
    handlers：
        用户可以在Vertex类中通过定义一个适当的handler函数来解决冲突；
    惰性协调机制，全局的拓扑改变在被apply之前不需要进行协调。对顶点V的修改引发的冲突由V自己来处理。

###3.5 Input and Output
    Reader、Writer


##4.Implementation
    一个图计算任务会被分解到多台机器上同时执行:
        集群管理系统，命名服务系统，临时文件本地磁盘缓存，持久化到分布式文件系统；

###4.1 Basic architecture
    顶点partition:
        每个分区包含一部分顶点以及以其为起点的边；
        默认partition函数为hash(vertexID) mod NumMachine；
        
    Pregel程序执行过程：
        ①用户程序的多个copy开始在集群中的机器上执行。一Master协调，多Workers执行任务；
        ②Master分配一个或多个partition给每个Worker节点，Worker维护状态、执行Compute函数、接收发送消息，每一个worker都知道该图的计算在所有worker中的分配情况；
        ③Master进程为每个worker分配用户输入中的一部分（顶点和边的记录集合）。对输入的划分和对整个图的划分是正交的，通常都是基于文件边界进行划分。如果一个worker加载的顶点刚好是这个worker所分配到的那一部分，那么相应的数据结构就会被立即更新。否则，该worker就需要将它发送到它所应属于的那个worker上。当所有的输入都被load完成后，所有的顶点将被标记为active状态
        ④Master给每个worker发指令，让其运行一个superstep，worker会为每个partition启动一个线程，轮询在其之上的顶点。调用每个active顶点的Compute()函数，传递给它从上一次superstep发送来的消息。消息是被异步发送的，这是为了使得计算和通信可以并行，以及batching，但是消息的发送会在本superstep结束前完成。当一个worker完成了其所有的工作后，会通知master，并告知当前该worker上在下一个superstep中将还有多少active节点。不断重复该步骤，只要有顶点还处在active状态，或者还有消息在传输。 
        ⑤计算结束后，master会给所有的worker发指令，让它保存它那一部分的计算结果。

###4.2 Fault Tolerance
    checkpointing容错。
    在每个superstep的开始阶段，master命令worker让它保存它上面的partitions的状态到持久存储设备，包括顶点值，边值，以及接收到的消息。Master自己也会保存aggregator的值；
    心跳，恢复时回滚到最近的checkpoint，并所有的!partition的supersteps重新执行；
    Confined Recovery：
        worker同时还会将其在加载图的过程中和superstep中发送出去的消息写入日志。这样恢复就会被限制在丢掉的那些partitions上；

###4.3 Worker implementation
    一个worker机器会在内存中维护分配到其之上的graph partition的状态。
    (vertexID) map (vertexValue,outgoingEdges,incomingMessageQueue,activeFlag)；
    该worker在每个superstep中，会循环遍历所有顶点，并调用每个顶点的Compute()函数、传给该函数顶点的当前值、一个接收到的消息的迭代器和一个出边的迭代器。
    性能：
        标志顶点是否为active的标志位是和输入消息队列分开保存的；
        另外，只保存了一份顶点值和边值，但有两份顶点active flag和输入消息队列存在，一份是用于当前superstep，另一个用于下一个superstep。当一个worker在进行superstep S的顶点处理时，同时还会有另外一个线程负责接收从处于同一个superstep的其他worker接收消息。由于顶点当前需要的是superstep S-1的消息，那么对superstep S和superstep S+1的消息就必须分开保存。类似的，顶点V接收到了消息表示V将会在下一个superstep中处于active，而不是当前这一次；
        当Compute()请求发送一个消息到其他顶点时，worker首先确认目标顶点是属于远程的worker机器，还是当前worker。如果是在远程的worker机器上，那么消息就会被缓存，当缓存大小达到一个阈值，最大的那些缓存数据将会被异步地flush出去，作为单独的一个网络消息传输到目标worker。如果是在当前worker，那么就可以做相应的优化：消息就会直接被放到目标顶点的输入消息队列中；
        如果用户提供了Combiner，那么在消息被加入到输出队列或者到达输入队列时，会执行combiner函数。后一种情况并不会节省网络开销，但是会节省用于消息存储的空间；

###4.4 Master implementation
    协调workers工作，workerID、worker IP地址、worker portion；
    Master保存这些信息只与分区数量有关，故只有一个Master也能承担大规模图计算协调任务；
    Barrier Synchronize：
        可用ZooKeeper实现，每一次同步是一个superstep的完成和下一个superstep的开始；
    Master同时还保存着整个计算过程以及整个graph的状态的统计数据，如图的总大小，关于出度分布的柱状图，处于active状态的顶点个数，在当前superstep的时间信息和消息流量，以及所有用户自定义aggregators的值等。为方便用户监控，Master在内部运行了一个HTTP服务器来显示这些信息；

###4.5 Aggregators
    每个Aggregator会通过对一组value值集合应用aggregation函数计算出一个全局值。每一个worker都保存了一个aggregators的实例集，由type name和实例名称来标识。当一个worker对graph的某一个partition执行一个superstep时，worker会combine所有的提供给本地的那个aggregator实例的值到一个local value：即利用一个aggregator对当前partition中包含的所有顶点值进行局部规约。在superstep结束时，所有workers会将所有包含局部规约值的aggregators的值进行最后的汇总，并汇报给master。这个过程是由所有worker构造出一棵规约树而不是顺序的通过流水线的方式来规约，这样做的原因是为了并行化规约时cpu的使用。在下一个superstep开始时，master就会将aggregators的全局值发送给每一个worker。


##5.Application
    PageRank、最短路径、二部图匹配、Semi-Clusting算法；





#From "Think Like a Vertex" to "Think Like a Graph"
##0.Abstract
    Giraph++：an open source implementation of Pregel.
    vertex-centric -->  graph-centric


##1.Introduction
    单节点并行图分区算法  --图数据量激增-->  需要大规模分布式图分区解决方案；
    MapReduce大规模并行计算模型不适合处理迭代计算（外部loop）；
    
    现有的大多数图处理系统的共性：
        ①将输入图划分为并行的partitions；
        ②使用vertex-centric编程模型，围绕vertex表达算法；
    vertex-centric缺点：
            在vertex-centric模型中每个vertex只有它邻居顶点的信息，因此信息在graph中一次一跳、缓慢地蔓延，即使在同一个分区中也需要将信息一步步地从source传到destination；
    graph partitions：
        每个graph分区代表一个适当的原始输入图的子图，而不是一些没有关系的vertex集合；


##2.Giraph++：Graph-Centric Model
###2.1 Internal Vertices and Boundary Vertices
     Vi = Pi ∪ {v|(u,v)∈E ∧ u∈Pi}；
     其中Pi代表internal vertex集合，也称为顶点的owner；  只存在于一个子图中；
     Vi/Pi代表boundary vertex集合；  可以在0个或多个子图中；
    
    internal vertex：
        拥有vertex value、edge values、incoming messages
    boundary vertex：
        只有一个关联的vertex value;
        temporary local copy  --local compute and propagate to-->  primary copy in it's corresponding internal vertex；
    
    消息传递只发生在boundary vertices传到它们的primary copies：
        ①internal vertices之间的信息交换是廉价且立即的，不需要网络消息或等待下一个superstep；
        ②boundary vertices之间需要通过网络消息将改变扩散到owners，且需要在一个superstep结束的时候；

###2.2 Giraph++ API
    依旧按被全局同步barriers分隔的supersteps序列执行，但计算执行在一个分区中的整个子图上；
    Vertex Class
    GraphPartition Class
    需要更少的messages和supersteps；


##3. Exploring Giraph++
###3.1 Graph Traversal
    Connected Component Algorithm

###3.2 Random Walk
    PageRank

###3.3 Graph Aggregation

