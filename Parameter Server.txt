[TOC]
#Parameter Server
##0.Abstract 
    数据和工作负载在worker节点、server节点维护全局共享参数(向量或矩阵)
    异步数据通信(没有同步的木桶效应)
    灵活的一致性模型(Sequential、Eventual、Bounded Delay:trade-off between system efficiency fast algorithm convergence rate)
    弹性的扩展能力(一致性哈希，新的Server可以随时动态插入集合中，无需重新运行系统)
    持续的容错(server备份、worker重启)


##1.Introduction
    分布式优化和推理成为了大规模机器学习的先决条件，单机已无法解决快速增长的数据和参数。
    训练数据的数量可能达到1TB到1PB之间，而训练过程中的参数可能会达到10^9~10^12，且这些共享参数需要被所有worker节点频繁访问，挑战如下：
        巨量参数，需要大量网络带宽；
        迭代计算，由于synchronize barriers，最慢的机器会拖慢整体；
        容错

###1.1 Contributions
    提供5个特征：
        ①高效通信：异步通信模型；
        ②灵活一致性模型：隐藏了同步的代价和延迟；
        ③弹性可扩展：动态扩容；
        ④容错和持久性：Vector clocks保证了网络分区和失败后的良好表现；
        ⑤易用性：用向量和矩阵表示全局共享参数；

###1.2 Engineering Challenges
    每个server节点只维护一部分参数，每个worker节点也只请求一部分参数；
    2个挑战：
        ①Communication：
            参数可以表示成KV对，但每个更新都发送KV操作开销太大；
            许多机器学习算法的参数可以用向量、矩阵、张量表示，每次更新workers只需发送向量或矩阵的一部分即可；
        ②FaultTollerance:
            Live replication of parameters between servers supports hot failover；
            continuous faulttolerance；

###1.3 Related Work
    第一代：使用memcached(key, value) 键值对存储作为同步机制；YahooLDA改进这个机制，增加了一个专门的服务器，提供用户可自定义的更新操作(set, get, update)；
    第二代：用bounded delay模型改进YahooLDA，但进一步限制了worker线程模型；
    第三代：能够解决这些局限性。

    parameter server与通用分布式系统的区别：
        通用的分布式系统通常都是：每次迭代都强制同步，通常在几十个节点上，它们的性能可以表现的很好，但是在大规模集群中，这样的每次迭代强制同步的机制会因为木桶效应变得很慢；
        Mahout基于Hadoop，MLI基于Spark，它们（Spark与MLI）采用的都是Iterative MapReduce的架构。它们能够保持迭代之间的状态，并且执行策略也更加优化了。但是，由于这两种方法都采用同步迭代的通信方式，容易因为个别机器的低性能导致全局性能的降低。
    
        为了解决这个问题，Graphlab采用图形抽象的方式进行异步调度通信。但是它缺少了MapReduce-Based的弹性扩展性，并且它使用粗粒度的snapshots来进行恢复，这两点都会阻碍到可扩展性。parameter server正是吸取Graphlab异步机制的优势，并且解决了其在可扩展性方面的劣势；
        Piccolo用了一个策略来共享和聚合状态：Workers本地pre-aggregate state，然后将updates传输给Server；


##2.Machine Learning
    机器学习广泛使用在：互联网搜索、爬虫监测、推荐系统、计算广告、文档分析；
    训练数据：特征抽取、目标函数、学习；

###2.1 Goals
    objective function；
    迭代计算直到最优或收敛；
    巨大的训练数据集和模型参数：trillions of trillion-length feature vectors and dimensions；

###2.2 Risk Minimization
    Risk是指对预测偏差的度量；
    训练数据过少、模型太复杂 --> 过拟合，模型太简单 --> 抓取不到正确决策的相关信息；
    经验风险函数 = 损失函数 + 模型复杂度；
    
    例子：分布式梯度下降

###2.3 Generative Models
    unsupervised algorithms
    
    例子：主题模型


##3.Architecture
    在parameter server中，每个server只负责分到的部分参数（servers共同维持一个全局的共享参数），每个worker也只分到部分数据和处理任务；
    
    1个Server Group：n个server节点、1个server manager
        server节点：
            每个server节点只负责全局共享参数的一部分，server节点间相互通信以备份参数增强可靠性或迁移参数增强可扩展性；
        server manager节点：
            拥有所有server节点的元数据(如各个节点的存活状态，参数分区的分配情况等)的一致性视图；
    
    n个Worker Group：每个group有n个worker节点、1个task scheduler
        worker节点：
            每个worker节点只存储训练数据的一部分以进行本地统计计算(如梯度)；worker节点之间没有通信，只与server节点进行通信以更新和拉取共享的参数。
        task scheduler：
            每个worker group有一个task scheduler，负责向worker分配任务，并且监控worker的运行情况。当有新的worker加入或者退出，task scheduler负责重新分配未完成的任务。

###3.1 (Key, Value) Vectors
    最小化损失函数的问题，key就是featureID，而value就是它的权值；
    LDA问题，key是wordID+topicID，而value就是一个计数值；
    不存在的keys权值为0；
    大多数的已有的框架都是这么对(key, value)进行抽象的。但是PS框架除此之外还把这些(k,v)对看作是稀疏的线性代数对象（通过保证key是有序的情况下），因此在对vector进行计算操作的时候，也会在某些操作上使用BLAS库等线性代数库进行优化;
    keys are ordered --> 将参数看作(K,V)对，赋予他们向量、矩阵的语义；

###3.2 Range Push and Pull
    workers与servers之间通过push和pull来通信。workers通过push将计算好的参数发送到server，然后通过pull从server更新参数。
    为了提高计算性能和带宽效率，parameter server允许用户使用Range Push 和 Range Pull操作：
        假设R是需要push或pull的key的range
        w.push(R, dest)
        w.pull(R, dest) 

###3.3 User-Defined Functions on Server
    server节点除了从workers聚合数据，还可以执行用户定义的函数（server节点有更完整的或最新的共享参数信息 --> 更新模型参数）；

###3.4 Asynchronous Tasks and Dependency
    rpc发起tasks，可以是worker对servers发起的push、pull，也可以是scheduler发起的用户自定义函数。tasks可能含有任意数量的子tasks；
    
    任务异步执行：
        调用者标记一个任务已完成，当它收到被调用者的回复（用户自定义函数的返回、pull的键值对请求、空ack）；
        被调用者标记一个任务已完成，当它的调用已返回并且所有的子任务已完成；
        
    默认为了性能被调用者并行执行tasks，但是如果调用者希望串行执行tasks的话可以在两个tasks之间添加execute-after-finished依赖，如:
        等所有worker的参数都被聚合了，server再聚合参数；
        支持灵活的一致性模型；

###3.5 Flexible Consistency
    独立的任务通过并行使用CPU、磁盘、网络带宽提高了系统的效率，但这可能会导致节点间数据的不一致性，从而(使用旧的模型参数计算梯度)降低算法的收敛速率；
    "系统性能"与"算法收敛速率"之间存在一个"trade-off"：
        算法对于参数非一致性的敏感度；
        训练数据特征之间的关联度；
        硬盘的存储容量；
    
    考虑到用户使用的时候会有不同的情况，parameter server为用户提供了多种任务依赖方式：
        ①Sequential：
            任务顺序执行，前一个任务结束后一个任务才能开始，和单线程实现对等，也称为Bulk Synchronous Processing（BSP）；
        ②Eventual：
            所有任务可以并行开始；
        ③Bounded Delay：
            是sequential与eventual之间的trade-off，可以设置一个τ作为最大的延时时间。也就是说，只有τ时间之前的任务都被完成了，才能开始一个新的任务；
            τ=0的情况就是Sequential一致性模型； 
            τ=∞的情况就是Eventual一致性模型；
            
        注意，依赖图可以是动态的。比如调度器可以根据运行时进程来增加或减少最大延迟，以平衡系统性能和算法收敛速率。

###3.6 User-defined Filters
    为了弥补基于调度器的流控，parameter server支持用户定义的过滤器来选择性地同步独立的键值对，实现一个任务内数据一致性的细粒度的控制；
    如significantly modified filter；


##4.Implementation
    servers使用一致性哈希存储模型参数(key-value pairs)；
    链备份容错；
    range-based通信、range-based vector clocks；

###4.1 Vector Clock
    背景：复杂的任务依赖图和快速恢复；
    每个key-value pair都关联了一个vector clock，parameter server使用vector clock来记录每个节点中参数的时间戳，能够用来跟踪聚合数据的状态或避免数据的重复发送(见Server Management)；
    但是，假设有n个节点，m个参数，一种naive的vector clock实现的空间复杂度是O(n∗m)，当有几千个节点和几十亿的参数时，内存和带宽都无法承受；
    好在，parameter server在push和pull的时候，都是rang-based，这就带来了一个好处：这个range里面的参数共享的是同一个时间戳，这显然可以大大降低空间复杂度；

###4.2 Messages
    一条消息包括一系列key-value pairs、key range R和相关的range vector clock：[vc(R),(k1,v1),...,(kp,vp)] kj∈R and j∈{1,2...p}；
    这是parameter server中最基本的通信格式，不仅仅是共享的参数，task的message也是这样的格式，只要把这里的(key,value)改成(task ID,参数/返回值)；
    
    由于机器学习问题通常都需要很高的网络带宽，因此信息的压缩是必须的:
    ①key-caching：因为训练数据通常在分配之后都不会发生改变，因此worker没有必要每次都发送相同的key，只需要接收方在第一次接收的时候缓存起来就行了，第二次，worker不再需要同时发送key和value，只需要发送value和key list的hash值就行，这样瞬间减少了一半的通信量；
    ②value-compression：假设参数是稀疏的，那么就会有大量的0存在，因此，为了进一步压缩，我们只需要发送非0值。parameter server使用Snappy快速压缩库来压缩数据、高效去除0值；
        用户自定义过滤： 
            对于机器学习优化问题比如梯度下降来说，并不是每次计算的梯度对于最终优化都是有价值的，用户可以通过自定义的规则过滤一些不必要的传送，再进一步压缩带宽cost： 
            1. 发送很小的梯度值是低效的： 
                因此可以自定义设置，只在梯度值较大的时候发送； 
            2. 更新接近最优情况的值是低效的： 
                因此，只在非最优的情况下发送，可通过KKT来判断；
    注意，key-caching和value-compression可以同时使用；
    
###4.3 Consistent Hashing
    一致性哈希，server节点id和key都插入一个hash ring，每个server节点管理从它的插入点逆时针到下一个插入点之间的key；
    server manager处理ring management；其它节点缓存key分区到本地；
    
###4.4 Replication and Consistency
    每个server节点都存储了它逆时钟方向的k个节点中的key的备份。图中，k=2，S1备份了S2和S3内的key；
    
    两种方式保证slave跟master之间的数据一致性：
    ①Chain replication(链备份，强一致性)：
        a. 更新：只能发生在数据头节点,然后更新逐步后移，直到更新到达尾节点，并由尾节点向客户确认更新成功； 
        b. 查询：为保证强一致性，客户查询只能在尾节点进行；
    ②Replication after Aggregation(延迟备份直至聚合)：
        两个worker节点分别向server传送x和y，server首先通过一定方式，如f(x+y)进行aggregate，然后再进行备份操作；
        当有n个worker的时候，复制只需要k/n的带宽。通常来说，k(备份数量)是一个很小的常数，而n的值大概是几百到几千；

###4.5 Server Management
    要实现系统的容错和动态的扩展系统规模，必须要求系统能够支持动态添加和移除节点；
    
    添加一个server节点 ：
        1. server manager会为新的节点分配相应range的key-value pairs和vector clocks使其充当一个master，这会造成其他server节点的key的变化； 
        2. 新添节点会获取数据并维护自己为一个master，并备份k份到slaves； 
        3. server manager将节点变化情况广播出去，接收方可能会移除不再属于自己的key-value pairs和vc(R)，并且将未完成的任务提交给新添的节点；
    
    删除一个server节点：
        server manager通过心跳信号确定一个server死亡，然后就会将该server的key range分配给另一个节点；

###4.6 Worker Management
    添加一个worker节点：
        1. task scheduler为新的节点W分配数据； 
        2. 这个worker节点通过网络或者文件系统加载分配到的训练数据，接着，W会从服务器pull参数； 
        3. task scheduler会广播节点的变化情况，可能会使一些节点释放一部分训练数据；
        
    删除一个worker节点：
        删除worker通常直接不管该节点，因为丢失一部分训练数据通常并不会影响模型训练结果，但是恢复一个worker可能占用较多资源；
        当然用户也可以选择用新的worker来替代丢失的worker；
    
    
    
    
    
# ps-lite框架是DMLC组自行实现的parameter server通信框架，是DMLC其它项目的核心，例如其深度学习框架MXNET就依赖了ps-lite的实现。
